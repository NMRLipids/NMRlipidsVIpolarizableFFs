{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation description file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOI=\"10.5281/zenodo.3727549\"\n",
    "def_file  = \"/home/samuli/work/NMRlipids/MATCH/scripts/orderParm_defs/order_parameter_definitions_MODEL_CHARMM36_popc.def\"\n",
    "\n",
    "#user_information = \"\"\"\n",
    "#DOPE test\n",
    "##NMRLIPIDS BEGIN\n",
    "#\n",
    "#@SIM\n",
    "#@SOFTWARE=gromacs\n",
    "#@FF=CHARMM36\n",
    "#@FF_SOURCE=CHARMM-GUI\n",
    "#@FF_DATE=??\n",
    "#@TRJ=dope.trr\n",
    "#@TPR=dope.tpr\n",
    "##\n",
    "##NMRLIPIDS END\n",
    "#\n",
    "#\"\"\"\n",
    "\n",
    "user_information = \"\"\"\n",
    "DOPE test\n",
    "#NMRLIPIDS BEGIN\n",
    "\n",
    "@SIM\n",
    "@SOFTWARE=gromacs\n",
    "@FF=CHARMM36\n",
    "@FF_SOURCE=CHARMM-GUI\n",
    "@FF_DATE=??\n",
    "@TRJ=dope.trr\n",
    "@TPR=dope.tpr\n",
    "\n",
    "#NMRLIPIDS END\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Working directory\n",
    "dir_wrk  = \"/home/samuli/work/temp/DATAbaseTST/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with files and directories\n",
    "import os\n",
    "\n",
    "#For quering webs\n",
    "import urllib.request\n",
    "from urllib.error import URLError,HTTPError\n",
    "\n",
    "# From time monitoring\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Python program to find SHA256 hash string of a file\n",
    "import hashlib\n",
    "\n",
    "# For dealing with excel and cvs \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "#To make real independent copies of lists\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/samuli/work/temp/DATAbaseTST\n",
      "/Users/peptid/Local_Documents/test/order_parameters.def\n"
     ]
    }
   ],
   "source": [
    "dir_wrk = os.path.normpath(dir_wrk)\n",
    "print(dir_wrk)\n",
    "def_file = os.path.normpath(def_file)\n",
    "print(def_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the DOI link is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://doi.org/10.5281/zenodo.3727549\n",
      "Status of the DOI link: OK\n"
     ]
    }
   ],
   "source": [
    "DOI_url = 'https://doi.org/' + DOI\n",
    "print(DOI_url)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(DOI_url)\n",
    "    print(\"Status of the DOI link: {0}\".format(response.msg))\n",
    "except HTTPError as e:\n",
    "    print(DOI_url)\n",
    "    print('The server couldn\\'t fulfill the request.')\n",
    "    print('Error code: ', e.code)\n",
    "    user_information = \"\"\n",
    "    print('The code will not proceed, please fix DOI')\n",
    "except URLError as e:\n",
    "    print(DOI_url)\n",
    "    print('We failed to reach a server.')\n",
    "    print('Reason: ', e.reason)\n",
    "    user_information = \"\"\n",
    "    print('The code will not proceed, please fix DOI')\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[{'TRJ': 'traj.dcd', 'SOFTWARE': 'namd', 'PDB': 'traj.pdb', 'ID': 0, 'FF': 'CHARMM36', 'FF_SOURCE': 'CHARMM-GUI'}]\n"
     ]
    }
   ],
   "source": [
    "bNMRLIPIDS = False #Check if the link contains NMRLIPIDS metadata\n",
    "nsims =0 # Counter number of simulations in a submission\n",
    "sims = [] #Array with the dictionary containing the information of a simulation\n",
    "\n",
    "for line in user_information.split(\"\\n\"):\n",
    "    if line.strip() == \"\":\n",
    "        continue\n",
    "    if \"#NMRLIPIDS BEGIN\" in line:\n",
    "        bNMRLIPIDS = True\n",
    "        continue\n",
    "    if \"#NMRLIPIDS END\" in line:\n",
    "        bNMRLIPIDS = False\n",
    "        continue\n",
    "    if \"@SIM\" in line:\n",
    "        #sims.append({\"ID\" : nsims, \"STATUS\" : 0})\n",
    "        sims.append({\"ID\" : nsims})\n",
    "        nsims += 1\n",
    "        continue\n",
    "    if not bNMRLIPIDS:\n",
    "        continue\n",
    "    if line.strip()[0] == \"@\":\n",
    "        key, value = line.split(\"=\")\n",
    "        sims[-1][key.strip('@')] = value\n",
    "print(nsims)\n",
    "print(sims)      \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CHARMM', 'AMBER', 'GROMACS', 'OPENMM', 'NAMD'])\n"
     ]
    }
   ],
   "source": [
    "# Gromacs\n",
    "gromacs_dict = {\n",
    "               'INI' : {\"REQUIRED\": False,\n",
    "                        \"TYPE\" : \"files\",\n",
    "                        \"EXTENSION\" : (\"gro\", \"pdb\",),\n",
    "                       }, # Could be not needed in the future (tpr)\n",
    "               'MDP' : {\"REQUIRED\": False,\n",
    "                        \"TYPE\" : \"file\",\n",
    "                        \"EXTENSION\" : (\"mdp\",),\n",
    "                       }, # Could be not needed in the future (tpr)\n",
    "               'TRJ' : {\"REQUIRED\": True,\n",
    "                        \"TYPE\" : \"files\",\n",
    "                        \"EXTENSION\" : (\"xtc\",\"trr\",),\n",
    "                       },\n",
    "               'TPR' : {\"REQUIRED\": True,\n",
    "                        \"TYPE\" : \"file\",\n",
    "                        \"EXTENSION\" : (\"tpr\",),\n",
    "                       },\n",
    "               'CPT' : {\"REQUIRED\": False,\n",
    "                        \"TYPE\" : \"file\",\n",
    "                        \"EXTENSION\" : (\"cpt\",),\n",
    "                       },\n",
    "               'TOP' : {\"REQUIRED\": False,\n",
    "                        \"TYPE\" : \"file\",\n",
    "                        \"EXTENSION\" : (\"top\",),\n",
    "                       },\n",
    "               'ITP' : {\"REQUIRED\": False,\n",
    "                        \"TYPE\" : \"files\",\n",
    "                        \"EXTENSION\" : (\"itp\",),\n",
    "                       },\n",
    "               'FF'  : {\"REQUIRED\": False,\n",
    "                        \"TYPE\" : \"string\",\n",
    "                       },\n",
    "               'FF_SOURCE' : {\"REQUIRED\": False,\n",
    "                              \"TYPE\" : \"string\",\n",
    "                              },\n",
    "               'FF_DATE' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"date\",\n",
    "                           },\n",
    "               }\n",
    "\n",
    "# Amber\n",
    "amber_dict = {}\n",
    "\n",
    "# NAMD\n",
    "namd_dict = {   \n",
    "            'TRJ' : { \"REQUIRED\": True,\n",
    "                      \"TYPE\": \"files\",\n",
    "                      \"EXTENSION\": (\"dcd\"),\n",
    "                    },\n",
    "            'INP' : { \"REQUIRED\": False,\n",
    "                      \"TYPE\": \"file\",\n",
    "                      \"EXTENSION\": (\".inp\"),\n",
    "                    },\n",
    "            'LOG' : { \"REQUIRED\": False,\n",
    "                      \"TYPE\": \"files\",\n",
    "                      \"EXTENSION\": (\"log\"),\n",
    "                      # can be parsed to get software version etc.\n",
    "                    },\n",
    "            'PSF' : { \"REQUIRED\": False,\n",
    "                      \"TYPE\": \"file\",\n",
    "                      \"EXTENSION\": (\"psf\"),\n",
    "                    },\n",
    "            'FF'  :  { \"REQUIRED\": False,\n",
    "                      \"TYPE\" : \"string\",\n",
    "                    },\n",
    "            'FF_SOURCE' : {\"REQUIRED\": False,\n",
    "                           \"TYPE\" : \"string\",\n",
    "                              },\n",
    "            'FF_DATE' : {\"REQUIRED\": False,\n",
    "                         \"TYPE\" : \"date\",\n",
    "                        },\n",
    "            'PDB'  : { \"REQUIRED\": True,\n",
    "                    \"TYPE\": \"file\",\n",
    "                    \"EXTENSION\": \"pdb\",}\n",
    "               }\n",
    "          \n",
    "# CHARMM\n",
    "charmm_dict = {}\n",
    "\n",
    "# OPENMM\n",
    "openmm_dict = {}\n",
    "\n",
    "# SOFTWARE\n",
    "software_dict = {\n",
    "                \"GROMACS\" : gromacs_dict, \n",
    "                \"AMBER\"   : amber_dict,\n",
    "                \"NAMD\"    : namd_dict,\n",
    "                \"CHARMM\"  : charmm_dict,\n",
    "                \"OPENMM\"  : openmm_dict,\n",
    "                }\n",
    "\n",
    "print(software_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check software used by the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 0 uses supported software NAMD and will be further processed\n"
     ]
    }
   ],
   "source": [
    "sims_valid_software = []\n",
    "for sim in sims:\n",
    "    if sim['SOFTWARE'].upper() in software_dict.keys():\n",
    "        msg_info = \"Simulation {0} uses supported software {1} and will be further processed\"\n",
    "        print (msg_info.format(sim['ID'], sim['SOFTWARE'].upper()))\n",
    "        sims_valid_software.append(sim.copy())\n",
    "    else:\n",
    "        msg_err=\"Simulation {0} performed in an UNSUPPORTED software {1} and will NOT be further processed\"\n",
    "        print(msg_err.format(sim[\"ID\"], sim[\"SOFTWARE\"].upper()))\n",
    "#print(sims_valid_software) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that all entry keys provided for each simulation are valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All entries in simulation 0 are understood and will be further processed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sims_valid_entries = []\n",
    "for sim in sims_valid_software:\n",
    "    #print(\"ID {0}\".format(sim[\"ID\"]))\n",
    "    wrong_key_entries = 0\n",
    "    software_dict_name = \"{0}_dict\".format(sim['SOFTWARE'].lower())\n",
    "    for key_sim, value_sim in sim.items():\n",
    "        #print(key_sim, value_sim)\n",
    "        if key_sim.upper() in (\"ID\", \"SOFTWARE\"):\n",
    "            #print(\"NOT REQUIRED\")\n",
    "            continue\n",
    "        if key_sim.upper() not in software_dict[sim['SOFTWARE'].upper()].keys():\n",
    "            print (\"{0} NOT in {1}\".format(key_sim, software_dict_name))\n",
    "            wrong_key_entries += 1\n",
    "    if wrong_key_entries:\n",
    "        print(\"Simulation {0} has {1} unknown entry/ies and won't be longer considered, please correct.\\n\".format(sim['ID'],wrong_key_entries))\n",
    "    else:\n",
    "        msg_info = \"All entries in simulation {0} are understood and will be further processed\\n\"\n",
    "        print (msg_info.format(sim['ID']))\n",
    "        sims_valid_entries.append(sim.copy())\n",
    "#print(sims_valid_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process entries with files information to contain file names in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0\n",
      "traj.dcd added to list\n",
      "traj.pdb added to list\n"
     ]
    }
   ],
   "source": [
    "sims_files_to_array = deepcopy(sims_valid_entries)\n",
    "\n",
    "for sim in sims_files_to_array:\n",
    "    print(\"ID {0}\".format(sim[\"ID\"]), flush=True)\n",
    "    software_sim = software_dict[sim['SOFTWARE'].upper()]\n",
    "    for key_sim, value_sim in sim.items():\n",
    "        try:\n",
    "            entry_type = software_sim[key_sim]['TYPE']\n",
    "            if \"file\" in entry_type:\n",
    "                if isinstance(value_sim, list): continue  \n",
    "                files_list = []\n",
    "                print(\"{0} added to list\".format(value_sim))\n",
    "                # Place filenames into arrays\n",
    "                for file_provided in value_sim.split(\";\"):\n",
    "                    files_list.append([file_provided.strip()])\n",
    "                sim[key_sim] = files_list\n",
    "        except: #It is notmal that fails for \"ID\" and \"SOFTWARE\"\n",
    "            continue\n",
    "#print(sims_files_to_array)\n",
    "#print(sims_valid_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for multiple files in entries that can only contain one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0\n"
     ]
    }
   ],
   "source": [
    "sims_valid_file_entries = []\n",
    "for sim in sims_files_to_array:\n",
    "    print(\"ID {0}\".format(sim[\"ID\"]), flush=True)\n",
    "    files_issues = 0\n",
    "    software_sim = software_dict[sim['SOFTWARE'].upper()]\n",
    "    for key_sim, value_sim in sim.items():\n",
    "        try:\n",
    "            entry_type = software_sim[key_sim]['TYPE']\n",
    "            if entry_type == \"file\"  and len(value_sim) > 1:\n",
    "                print(\"Multiple values found in {0} and only one allowed (Please correct):\\n {1}\".format(key_sim,value_sim))\n",
    "                files_issues += 1\n",
    "        except: #It is notmal that fails for \"ID\" and \"SOFTWARE\"\n",
    "            continue\n",
    "    if files_issues:\n",
    "        print(\"Sim {0} will be no longer processed\".format(sim[\"ID\"]))\n",
    "    else:\n",
    "        sims_valid_file_entries.append(sim.copy())\n",
    "#print(sims_valid_file_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the submitted simulation has rssion has all required files and information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0\n",
      "All required entries present.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sims_required_entries = []\n",
    "for sim in sims_valid_file_entries:\n",
    "    print(\"ID {0}\".format(sim[\"ID\"]))\n",
    "    missing_required_keys = 0\n",
    "    for key, value in software_dict[sim['SOFTWARE'].upper()].items():\n",
    "        if value[\"REQUIRED\"]:\n",
    "            try:\n",
    "                sim[key]\n",
    "            except:\n",
    "                print(\"Entry not found: {0} {1}\".format(key, value))\n",
    "                missing_required_keys += 1\n",
    "    if missing_required_keys:\n",
    "        print(\"{0} missing required entry/ies, please correct.\".format(missing_required_keys))\n",
    "        print(\"Entry with ID={0} will not be further processed.\\n\".format(sim[\"ID\"]))\n",
    "    else:\n",
    "        print(\"All required entries present.\\n\")\n",
    "        sims_required_entries.append(sim.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check status links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download link\n",
    "def download_link(doi, file):\n",
    "    if \"zenodo\" in doi.lower():\n",
    "        zenodo_entry_number = doi.split(\".\")[2]\n",
    "        return 'https://zenodo.org/record/' + zenodo_entry_number + '/files/' + file\n",
    "    else:\n",
    "        print (\"DOI provided: {0}\".format(doi))\n",
    "        print (\"Repository not validated. Please upload the data for example to zenodo.org\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0\n",
      "All links work.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(sims_required_entries)\n",
    "sims_working_links = []         \n",
    "for sim in sims_required_entries:\n",
    "    print(\"ID {0}\".format(sim[\"ID\"]))\n",
    "    wrong_links = 0\n",
    "    software_sim = software_dict[sim['SOFTWARE'].upper()]\n",
    "    for key_sim, value_sim in sim.items():\n",
    "        #print(\"key_sim = {0} => value_sim = {1}\".format(key_sim, value_sim))\n",
    "        try:\n",
    "            entry_type = software_sim[key_sim]['TYPE']\n",
    "            #print(\"entry_type = {0}\".format(entry_type))\n",
    "            if \"file\" in entry_type:\n",
    "                for file_provided in value_sim:\n",
    "                    #print(\"File={0}\".format(file_provided[0]))\n",
    "                    file_url = download_link(DOI, file_provided[0])\n",
    "                    if file_url == \"\":\n",
    "                        wrong_links += 1\n",
    "                        continue\n",
    "                    try:\n",
    "                        response = urllib.request.urlopen(file_url)\n",
    "                        #print(\"Status of the DOI link: {0}\".format(response.msg))\n",
    "                    except HTTPError as e:\n",
    "                        print(\"\\nkey={0} => file={1}\".format(key_sim, file_provided[0]))\n",
    "                        print(file_url)\n",
    "                        print('The server couldn\\'t fulfill the request.')\n",
    "                        print('Error code: ', e.code)\n",
    "                        wrong_links += 1\n",
    "                    except URLError as e:\n",
    "                        print(key_sim, file_provided[0])\n",
    "                        print(file_url)\n",
    "                        print('We failed to reach a server.')\n",
    "                        print('Reason: ', e.reason)\n",
    "                        wrong_links += 1\n",
    "                    else:\n",
    "                        pass\n",
    "        except: #It is notmal that fails for \"ID\" and \"SOFTWARE\"\n",
    "            continue\n",
    "    if wrong_links:\n",
    "        print(\"{0} link/s failed, please correct.\".format(wrong_links))\n",
    "        print(\"Sim={0} will not be further processed.\\n\".format(sim[\"ID\"]))\n",
    "    else:\n",
    "        print(\"All links work.\\n\")\n",
    "        sims_working_links.append(sim.copy())\n",
    "#print(sims_working_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download files from links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDB: 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "TRJ: 100%|██████████| 1/1 [01:25<00:00, 85.67s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create temporary directory where to download files and analyze them\n",
    "dir_tmp = os.path.join(dir_wrk, \"tmp/\")\n",
    "if (not os.path.isdir(dir_tmp)): os.mkdir(dir_tmp)\n",
    "\n",
    "for sim in sims_working_links:\n",
    "    print(\"ID {0}\".format(sim[\"ID\"]), flush=True)\n",
    "    software_sim = software_dict[sim['SOFTWARE'].upper()]\n",
    "    dir_sim = os.path.join(dir_tmp, str(sim[\"ID\"]))\n",
    "    if (not os.path.isdir(dir_sim)): os.mkdir(dir_sim)\n",
    "    for key_sim, value_sim in sim.items():\n",
    "        #print(\"key_sim = {0} => value_sim = {1}\".format(key_sim, value_sim))\n",
    "        try:\n",
    "            entry_type = software_sim[key_sim]['TYPE']\n",
    "            #print(\"entry_type = {0}\".format(entry_type))\n",
    "            if \"file\" in entry_type:\n",
    "                for file_provided in tqdm(value_sim, desc = key_sim):\n",
    "                    file_url = download_link(DOI, file_provided[0])\n",
    "                    file_name = os.path.join(dir_sim, file_provided[0])\n",
    "                    if (not os.path.isfile(file_name)):\n",
    "                        response = urllib.request.urlretrieve(file_url, file_name)\n",
    "        except: #It is normal that fails for \"ID\" and \"SOFTWARE\"\n",
    "            continue\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate hash downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0\n",
      "       NAME TYPE REQUIRED                                      HASH\n",
      "0  traj.dcd  TRJ     True  2e24b8413dc8907d81d36ec6aacac3732d4dd36a\n",
      "1  traj.pdb  PDB     True  228d698ab20ececc7102a4ce9af7a52c93e63d52\n",
      "\n",
      "['2e24b8413dc8907d81d36ec6aacac3732d4dd36a', '228d698ab20ececc7102a4ce9af7a52c93e63d52']\n",
      "\n",
      "[{'TRJ': [['traj.dcd', '2e24b8413dc8907d81d36ec6aacac3732d4dd36a']], 'SOFTWARE': 'namd', 'PDB': [['traj.pdb', '228d698ab20ececc7102a4ce9af7a52c93e63d52']], 'ID': 0, 'FF': 'CHARMM36', 'FF_SOURCE': 'CHARMM-GUI'}]\n"
     ]
    }
   ],
   "source": [
    "dir_tmp = os.path.join(dir_wrk, \"tmp/\")\n",
    "sims_hashes = deepcopy(sims_working_links)\n",
    "\n",
    "for sim in sims_hashes:\n",
    "    print(\"ID {0}\".format(sim[\"ID\"]), flush=True)\n",
    "    software_sim = software_dict[sim['SOFTWARE'].upper()]\n",
    "    dir_sim = os.path.join(dir_tmp, str(sim[\"ID\"]))\n",
    "    \n",
    "    #list_containing the sha1 sums for all required files\n",
    "    sha1_list_requied = []\n",
    "    \n",
    "    # Make empty dataframe with the desired columns\n",
    "    df_files = pd.DataFrame(columns=['NAME','TYPE','REQUIRED','HASH'])\n",
    "    \n",
    "    for key_sim, value_sim in sim.items():\n",
    "        #print(\"key_sim = {0} => value_sim = {1}\".format(key_sim, value_sim))\n",
    "        try:\n",
    "            entry_type = software_sim[key_sim]['TYPE']\n",
    "            #print(\"entry_type = {0}\".format(entry_type))\n",
    "            if \"file\" in entry_type:\n",
    "                files_list = []\n",
    "                for file_provided in value_sim:\n",
    "                    file_name = os.path.join(dir_sim, file_provided[0])\n",
    "                    sha1_hash = hashlib.sha1()\n",
    "                    with open(file_name,\"rb\") as f:\n",
    "                        # Read and update hash string value in blocks of 4K\n",
    "                        for byte_block in iter(lambda: f.read(4096),b\"\"):\n",
    "                            sha1_hash.update(byte_block)\n",
    "                        #print(file_provided, sha256_hash.hexdigest())\n",
    "                        df_files = df_files.append({\n",
    "                            \"NAME\":file_provided[0],\n",
    "                            \"TYPE\":key_sim,\n",
    "                            \"REQUIRED\": software_dict[sim['SOFTWARE'].upper()][key_sim]['REQUIRED'],\n",
    "                            \"HASH\":sha1_hash.hexdigest(),\n",
    "                        }, ignore_index=True)\n",
    "                    files_list.append([file_provided[0], sha1_hash.hexdigest()])\n",
    "                #Find the keys of the required files to calculate the master_hash \n",
    "                if software_dict[sim['SOFTWARE'].upper()][key_sim]['REQUIRED'] == True:\n",
    "                    sha1_list_requied.append(sha1_hash.hexdigest())\n",
    "                sim[key_sim] = files_list #Problematic\n",
    "        except: #It is notmal that fails for \"ID\" and \"SOFTWARE\"\n",
    "            continue\n",
    "    print(df_files)\n",
    "    print(\"\\n{0}\\n\".format(sha1_list_requied))      \n",
    "    # Calculate the hash of a file contaning the hashes of each of the required files\n",
    "    # This should be always invariant as it will be used unique identifier for a simualtion\n",
    "    # Note order the hashes of the required files before calculating the hash (That means that the required files cannot change)\n",
    "print(sims_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'TRJ': [['traj.dcd']], 'SOFTWARE': 'namd', 'PDB': [['traj.pdb']], 'ID': 0, 'FF': 'CHARMM36', 'FF_SOURCE': 'CHARMM-GUI'}]\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(sims_working_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to databank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../Data/Simulations/10.5281:zenodo.3727549.0’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "for sim in sims_working_links:\n",
    "    ID=sim.get('ID')\n",
    "    # SAMULI: I replace \"/\" with \":\" because the former cannot be used in trajectory name\n",
    "    DATAdir='../Data/Simulations/'+str(DOI.replace(\"/\",\":\"))+'.'+str(ID)\n",
    "    !mkdir {DATAdir}\n",
    "    # SAMULI: I am writin now in txt, but using json might be better in the future\n",
    "    outfileDICT=open(str(DATAdir)+'/README.md','w')\n",
    "    outfileDICT.write(str(sim))\n",
    "    outfileDICT.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis starts here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First dowload packages and calculate the correlation functions using gromacs tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting the trajectory into xtc\n",
      "Warning: 'topology' data from input file(s) will be discarded. output format only supports fields: 'xyz', 'time', 'step', 'box'\n",
      "converted 1000 frames, 17208 atoms \n",
      "trajectory conversion is completed\n",
      "                  :-) GROMACS - gmx trjconv, VERSION 5.1.2 (-:\n",
      "\n",
      "                            GROMACS is written by:\n",
      "     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   \n",
      " Aldert van Buuren   Rudi van Drunen     Anton Feenstra   Sebastian Fritsch \n",
      "  Gerrit Groenhof   Christoph Junghans   Anca Hamuraru    Vincent Hindriksen\n",
      " Dimitrios Karkoulis    Peter Kasson        Jiri Kraus      Carsten Kutzner  \n",
      "    Per Larsson      Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff \n",
      "   Erik Marklund      Teemu Murtola       Szilard Pall       Sander Pronk   \n",
      "   Roland Schulz     Alexey Shvetsov     Michael Shirts     Alfons Sijbers  \n",
      "   Peter Tieleman    Teemu Virolainen  Christian Wennberg    Maarten Wolf   \n",
      "                           and the project leaders:\n",
      "        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel\n",
      "\n",
      "Copyright (c) 1991-2000, University of Groningen, The Netherlands.\n",
      "Copyright (c) 2001-2015, The GROMACS development team at\n",
      "Uppsala University, Stockholm University and\n",
      "the Royal Institute of Technology, Sweden.\n",
      "check out http://www.gromacs.org for more information.\n",
      "\n",
      "GROMACS is free software; you can redistribute it and/or modify it\n",
      "under the terms of the GNU Lesser General Public License\n",
      "as published by the Free Software Foundation; either version 2.1\n",
      "of the License, or (at your option) any later version.\n",
      "\n",
      "GROMACS:      gmx trjconv, VERSION 5.1.2\n",
      "Executable:   /usr/bin/gmx\n",
      "Data prefix:  /usr\n",
      "Command line:\n",
      "  gmx trjconv -f /home/samuli/work/temp/DATAbaseTST/tmp/0/tmp_converted.xtc -s /home/samuli/work/temp/DATAbaseTST/tmp/0/traj.pdb -o /home/samuli/work/temp/DATAbaseTST/tmp/0/whole.xtc -pbc mol\n",
      "\n",
      "Will write xtc: Compressed trajectory (portable xdr format): xtc\n",
      "\n",
      "-------------------------------------------------------\n",
      "Program gmx trjconv, VERSION 5.1.2\n",
      "Source code file: /build/gromacs-z6bPBg/gromacs-5.1.2/src/gromacs/utility/cstringutil.c, line: 93\n",
      "\n",
      "Fatal error:\n",
      "An input file contains a line longer than 4096 characters, while the buffer passed to fgets2 has size 4096. The line starts with: 'REMARK   DATE:     3'\n",
      "For more information and tips for troubleshooting, please check the GROMACS\n",
      "website at http://www.gromacs.org/Documentation/Errors\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'or'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a011e3439a2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mtmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_corr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mfirstlipid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mlastlipid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mnumlipid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlastlipid\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfirstlipid\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'or'"
     ]
    }
   ],
   "source": [
    "from OrderParameter import *\n",
    "import warnings\n",
    "from corrtimes import *\n",
    "import subprocess\n",
    "import mdtraj\n",
    "#!cp corr_ftios_ind.sh {dir_wrk}\n",
    "for sim in sims_working_links:\n",
    "    trj=sim.get('TRJ')\n",
    "    tpr=sim.get('TPR')\n",
    "    ID=sim.get('ID')\n",
    "    \n",
    "    ext=trj[0:-3] # getting the trajectory extension\n",
    "    \n",
    "    # BATUHAN: Adding a few lines to convert the trajectory into .xtc using MDTRAJ\n",
    "    #          We will need users to install MDTRAJ in their system so that we can convert other trajectories into xtc\n",
    "\n",
    "    if ext != \"xtc\":\n",
    "        \n",
    " \n",
    "        print(\"converting the trajectory into xtc\")\n",
    "        \n",
    "        pdb = sim.get('PDB')\n",
    "        output_traj = str(dir_wrk) + '/tmp/' + str(ID) + '/' + 'tmp_converted.xtc'\n",
    "        input_traj = str(dir_wrk) + '/tmp/' + str(ID) + '/' + trj[0][0]\n",
    "        input_pdb = str(dir_wrk) + '/tmp/' + str(ID) + '/' + pdb[0][0]\n",
    "      \n",
    "        if os.path.isfile(output_traj): # when we're done with the converted trajectory we can simply remove it\n",
    "            !rm {output_traj}\n",
    "        \n",
    "        !echo System | mdconvert {input_traj} -o {output_traj} -t {input_pdb} --force # force overwrite\n",
    "        \n",
    "        xtc = str(dir_wrk) + '/tmp/' + str(ID) + '/' + 'tmp_converted.xtc'\n",
    "        tpr=input_pdb\n",
    "        \n",
    "        print(\"trajectory conversion is completed\")\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        xtc = {dir_wrk}/tmp/{ID}/{str(trj[0][0])} \n",
    "        tpr = str(dir_wrk) + '/tmp/' + str(ID) + '/' + str(tpr[0][0])\n",
    "    \n",
    "    #tprnowat=str(dir_wrk)+'/tmp/'+str(ID)+'/'+'nowater.tpr'\n",
    "    #xtcnowatwhole=str(dir_wrk)+'/tmp/'+str(ID)+'/'+'whole.xtc'\n",
    "    xtcwhole=str(dir_wrk)+'/tmp/'+str(ID)+'/'+'whole.xtc'\n",
    "    #trj='tmp/'+str(ID)+'/'+str(trj[0][0])\n",
    "    #tpr=str(dir_wrk)+'/tmp/'+str(ID)+'/'+str(tpr[0][0])\n",
    "    #print(def_file,tpr,trj)\n",
    "\n",
    "    # SAMULI: line below could be removed by giving output folder for corr_ftios.sh\n",
    "    #!mv *rotacf*xvg {dir_wrk}/tmp/{ID}/\n",
    "    # SAMULI: line below could are because find_OP does not work for folders\n",
    "    #!cp {dir_wrk}/tmp/{ID}/{str(trj[0][0])} ./\n",
    "    #!cp {dir_wrk}/tmp/{ID}/{str(tpr[0][0])} ./\n",
    "    #Hanne: shortening the traj here for debugging \n",
    "    !echo System | gmx trjconv -f {xtc} -s {tpr} -o {xtcwhole} -pbc mol \n",
    "    # SAMULI: I compromized the speed to generality. Not all systems automatically have non-water group\n",
    "    #!echo non-water | gmx trjconv -f {dir_wrk}/tmp/{ID}/{str(trj[0][0])} -t0 0 -s {dir_wrk}/tmp/{ID}/{str(tpr[0][0])} -o {xtcnowatwhole} -pbc mol \n",
    "    #!echo non-Water | gmx trjconv -f tmp.trr -s {dir_wrk}/tmp/{ID}/{str(tpr[0][0])} -o {dir_wrk}/tmp/{ID}/whole.xtc -pbc mol \n",
    "    #!echo non-Water | gmx convert-tpr -s {dir_wrk}/tmp/{ID}/{str(tpr[0][0])} -o {tprnowat}\n",
    "    #!rm tmp.trr \n",
    "\n",
    "    \n",
    "    \n",
    "    output_corr=!{dir_wrk}/corr_ftios_ind.sh {def_file} {tpr} {xtcwhole} {dir_wrk}/tmp/{ID}\n",
    "    #tmp=output_corr.split()\n",
    "    #print (output_corr)\n",
    "    tmp=output_corr[-1].split()\n",
    "    \n",
    "    firstlipid=int(tmp[-2])\n",
    "    lastlipid=int(tmp[-1])\n",
    "    numlipid=lastlipid-firstlipid+1    \n",
    "    \n",
    "    print (ID, numlipid)\n",
    "    \n",
    "    \n",
    "    OrdParam=find_OP(def_file,tpr,xtcwhole)\n",
    "    outfile=open(str(dir_wrk)+'/tmp/'+str(ID)+'/'+'times'+str(ID)+'.txt','w')\n",
    "    outfile_teffs=open(str(dir_wrk)+'/tmp/'+str(ID)+'/times'+str(ID)+'_teffs.txt','w')\n",
    "\n",
    "    \n",
    "    for i,op in enumerate(OrdParam.values()):\n",
    "        resops =op.get_op_res\n",
    "        (op.avg, op.std, op.stem) =op.get_avg_std_stem_OP\n",
    "        \n",
    "        outfile_ind=open(str(dir_wrk)+'/tmp/'+str(ID)+'/'+op.name+'times_perlipid.txt','w')\n",
    "        convs=[]\n",
    "        Teffs=[]\n",
    "        Teffs_area=[]\n",
    "        R1s=[]\n",
    "        \n",
    "        for j in range(firstlipid,lastlipid+1):\n",
    "            print (op.name, j)\n",
    "            corrfile=dir_wrk+'/tmp/'+str(ID)+'/'+str(j)+'_rotacf_'+op.name+\".xvg\"\n",
    "            if j==1:\n",
    "                out,times=read_data(corrfile)\n",
    "                fvals=np.asmatrix(out).T\n",
    "            else:\n",
    "                out, times=read_data(corrfile)\n",
    "                fvals=np.concatenate((fvals,np.asmatrix(out).T),axis=1)\n",
    "            #Hanne: change to calc only R1 here\n",
    "            Teff,tau_eff_area, R1, conv =calc_corrtime_noread(out,times, resops[j-1])\n",
    "            Teffs.append(Teff)\n",
    "            Teffs_area.append(tau_eff_area)\n",
    "            R1s.append(R1)\n",
    "            convs.append(conv)\n",
    "            \n",
    "            line=str(op.name)+\" \"+str(resops[j-1])+\" \"+str(Teff)+\" \"+str(tau_eff_area)+\" \"+str(R1)+\" \"+str(conv)+ '\\n'\n",
    "            outfile_ind.write(line)\n",
    "        outfile_ind.close()\n",
    "\n",
    "        line2=str(op.name)+\" \"+str(op.avg)+\" \"+str(op.stem)+\" \"+str(np.mean(Teffs))+\" \"+str(np.std(Teffs, ddof=1)/np.sqrt(len(Teffs)-1))+\" \"+str(np.mean(Teffs_area))+\" \"+str(np.std(Teffs_area,ddof=1)/np.sqrt(len(Teffs_area)-1))+\" \"+str(np.mean(R1s))+\" \"+str(np.std(R1s, ddof=1)/np.sqrt(len(R1s)-1))+\" \"+str(sum(convs)/float(len(convs)))+'\\n'\n",
    "        outfile.write(line2)\n",
    "\n",
    "        #analysis of Teff error starts here\n",
    "        means=np.mean(fvals,axis=1)\n",
    "        stems=np.std(fvals,axis=1,ddof=1)/np.sqrt(int(numlipid)-1)\n",
    "        teff,teff_min,teff_max=calc_corrtime_withee(times,means, stems, op.avg, op.stem)\n",
    "        line3=str(op.name)+\" \"+str(op.avg)+\" \"+str(teff)+\" \"+str(teff_min)+\" \"+str(teff_max)+'\\n'\n",
    "        outfile_teffs.write(line3)        \n",
    "        line=str(op.name)+\" \"+str(op.avg)+\" \"+str(Teff)+\" \"+str(tau_eff_area)+\" \"+str(R1)+'\\n'\n",
    "\n",
    "    !cp {str(dir_wrk)}'/tmp/'{str(ID)}'/times'{str(ID)}'.txt' {DATAdir}\n",
    "    outfile.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrdParam=find_OP(def_file,tpr,xtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('gamma1_1', <OrderParameter.OrderParameter at 0x7f8137b24c50>),\n",
       "             ('gamma1_2', <OrderParameter.OrderParameter at 0x7f8137b240f0>),\n",
       "             ('gamma1_3', <OrderParameter.OrderParameter at 0x7f8137b24278>),\n",
       "             ('gamma2_1', <OrderParameter.OrderParameter at 0x7f8137b245c0>),\n",
       "             ('gamma2_2', <OrderParameter.OrderParameter at 0x7f8137b24d68>),\n",
       "             ('gamma2_3', <OrderParameter.OrderParameter at 0x7f8137b88be0>),\n",
       "             ('gamma3_1', <OrderParameter.OrderParameter at 0x7f8137b88ac8>),\n",
       "             ('gamma3_2', <OrderParameter.OrderParameter at 0x7f8137b88da0>),\n",
       "             ('gamma3_3', <OrderParameter.OrderParameter at 0x7f8137b88e10>),\n",
       "             ('beta1', <OrderParameter.OrderParameter at 0x7f8137b88eb8>),\n",
       "             ('beta2', <OrderParameter.OrderParameter at 0x7f8137b888d0>),\n",
       "             ('alpha1', <OrderParameter.OrderParameter at 0x7f8137b88a90>),\n",
       "             ('alpha2', <OrderParameter.OrderParameter at 0x7f8137b93da0>),\n",
       "             ('g3_1', <OrderParameter.OrderParameter at 0x7f8137b93dd8>),\n",
       "             ('g3_2', <OrderParameter.OrderParameter at 0x7f8137b93cc0>),\n",
       "             ('g2_1', <OrderParameter.OrderParameter at 0x7f8137b84160>),\n",
       "             ('g1_1', <OrderParameter.OrderParameter at 0x7f8137b84668>),\n",
       "             ('g1_2', <OrderParameter.OrderParameter at 0x7f8137b844e0>),\n",
       "             ('palmitoyl_C2a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b84080>),\n",
       "             ('palmitoyl_C2b',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b844a8>),\n",
       "             ('palmitoyl_C3a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b1d5c0>),\n",
       "             ('palmitoyl_C3b',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b1d048>),\n",
       "             ('palmitoyl_C4a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b1d160>),\n",
       "             ('palmitoyl_C4b',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b7be48>),\n",
       "             ('palmitoyl_C5a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b7bf28>),\n",
       "             ('palmitoyl_C5b',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b7bcf8>),\n",
       "             ('palmitoyl_C6a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b7be80>),\n",
       "             ('palmitoyl_C6b',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b8c0f0>),\n",
       "             ('palmitoyl_C7a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b8c048>),\n",
       "             ('palmitoyl_C7b',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b72d68>),\n",
       "             ('palmitoyl_C8a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b72048>),\n",
       "             ('palmitoyl_C8b',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b720b8>),\n",
       "             ('palmitoyl_C9a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b723c8>),\n",
       "             ('palmitoyl_C9b',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b72240>),\n",
       "             ('palmitoyl_C10a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b72da0>),\n",
       "             ('palmitoyl_C10b',\n",
       "              <OrderParameter.OrderParameter at 0x7f81377ebf28>),\n",
       "             ('palmitoyl_C11a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b77320>),\n",
       "             ('palmitoyl_C11b',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b772b0>),\n",
       "             ('palmitoyl_C12a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b8f1d0>),\n",
       "             ('palmitoyl_C12b',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b8f2e8>),\n",
       "             ('palmitoyl_C13a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b8f438>),\n",
       "             ('palmitoyl_C13b',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b8fb00>),\n",
       "             ('palmitoyl_C14a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b8f4e0>),\n",
       "             ('palmitoyl_C14b',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b8f860>),\n",
       "             ('palmitoyl_C15a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b8f828>),\n",
       "             ('palmitoyl_C15b',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b8f518>),\n",
       "             ('palmitoyl_C16a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b8fef0>),\n",
       "             ('palmitoyl_C16b',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b8fcc0>),\n",
       "             ('palmitoyl_C16c',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b8fe10>),\n",
       "             ('oleoyl_C2a', <OrderParameter.OrderParameter at 0x7f8137b8fac8>),\n",
       "             ('oleoyl_C2b', <OrderParameter.OrderParameter at 0x7f8137b8feb8>),\n",
       "             ('oleoyl_C3a', <OrderParameter.OrderParameter at 0x7f8137b8fda0>),\n",
       "             ('oleoyl_C3b', <OrderParameter.OrderParameter at 0x7f8137b6ffd0>),\n",
       "             ('oleoyl_C4a', <OrderParameter.OrderParameter at 0x7f8137b6fc18>),\n",
       "             ('oleoyl_C4b', <OrderParameter.OrderParameter at 0x7f8137b6fbe0>),\n",
       "             ('oleoyl_C5a', <OrderParameter.OrderParameter at 0x7f8137b6f1d0>),\n",
       "             ('oleoyl_C5b', <OrderParameter.OrderParameter at 0x7f8137b6fac8>),\n",
       "             ('oleoyl_C6a', <OrderParameter.OrderParameter at 0x7f8137b6fcc0>),\n",
       "             ('oleoyl_C6b', <OrderParameter.OrderParameter at 0x7f8137b6f390>),\n",
       "             ('oleoyl_C7a', <OrderParameter.OrderParameter at 0x7f813f4e6438>),\n",
       "             ('oleoyl_C7b', <OrderParameter.OrderParameter at 0x7f813f4e6588>),\n",
       "             ('oleoyl_C8a', <OrderParameter.OrderParameter at 0x7f813f4e6be0>),\n",
       "             ('oleoyl_C8b', <OrderParameter.OrderParameter at 0x7f813f4e6d30>),\n",
       "             ('oleoyl_C9a', <OrderParameter.OrderParameter at 0x7f813f4e6748>),\n",
       "             ('oleoyl_C10a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b28048>),\n",
       "             ('oleoyl_C11a',\n",
       "              <OrderParameter.OrderParameter at 0x7f813ea01cf8>),\n",
       "             ('oleoyl_C11b',\n",
       "              <OrderParameter.OrderParameter at 0x7f813ea01be0>),\n",
       "             ('oleoyl_C12a',\n",
       "              <OrderParameter.OrderParameter at 0x7f813ea01208>),\n",
       "             ('oleoyl_C12b',\n",
       "              <OrderParameter.OrderParameter at 0x7f813ea01ba8>),\n",
       "             ('oleoyl_C13a',\n",
       "              <OrderParameter.OrderParameter at 0x7f813ea01f28>),\n",
       "             ('oleoyl_C13b',\n",
       "              <OrderParameter.OrderParameter at 0x7f813ea012b0>),\n",
       "             ('oleoyl_C14a',\n",
       "              <OrderParameter.OrderParameter at 0x7f813ea01978>),\n",
       "             ('oleoyl_C14b',\n",
       "              <OrderParameter.OrderParameter at 0x7f813ea01dd8>),\n",
       "             ('oleoyl_C15a',\n",
       "              <OrderParameter.OrderParameter at 0x7f813ea01358>),\n",
       "             ('oleoyl_C15b',\n",
       "              <OrderParameter.OrderParameter at 0x7f813ea01da0>),\n",
       "             ('oleoyl_C16a',\n",
       "              <OrderParameter.OrderParameter at 0x7f813ea017f0>),\n",
       "             ('oleoyl_C16b',\n",
       "              <OrderParameter.OrderParameter at 0x7f813ea01898>),\n",
       "             ('oleoyl_C17a',\n",
       "              <OrderParameter.OrderParameter at 0x7f813ea01ef0>),\n",
       "             ('oleoyl_C17b',\n",
       "              <OrderParameter.OrderParameter at 0x7f813ea01748>),\n",
       "             ('oleoyl_C18a',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b6dda0>),\n",
       "             ('oleoyl_C18b',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b6d0b8>),\n",
       "             ('oleoyl_C18c',\n",
       "              <OrderParameter.OrderParameter at 0x7f8137b6d390>)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrdParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
